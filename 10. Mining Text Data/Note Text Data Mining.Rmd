---
title: "Mining Text Data"
author: "Hazim Fitri"
date: "2025-01-05"
output: 
  pdf_document:
    toc: true
    toc_depth: 6
---

Unstructured Data Analytic

NLP

-   Computer procedure to interpret unstructured data to understand human language.

Objective :

-   Identify trends, popular topics and themes relate to some particular issue

-   Extract sentiment and people's emotions towards some particular issue

Text corpus

-   Text need to be converted into corpus format for analysis

-   Data mining analysis, hypothesis testing, checking occurrences, validating linguistic rules can be done on corpus format.

Data Cleaning

-   Remove special character (/, \@, \|)

-   Convert text to lower case

-   Remove numbers

-   Remove stopwords (the, is, at, on)

-   Remove punctuation

-   Eliminate extra unnecessary spaces

Word tokenization

Text stemming

Document-term matrix

Word cloud

Word association

Sentiment analysis

Emotion classification

-   Built on the NRC Word-Emotion Association Lexicon

-   8 basic emotions :

    -   Anger

    -   Fear

    -   Anticipation

    -   Trust

    -   Surprise

    -   Sadness

    -   Joy

    -   Disgust

# Steps of cleaning text data.

1.  Remove special characters from the text, where a symbol such as; /, \@ and \| will be replace by a space.
2.  Convert the text to lower case.
3.  Remove numbers
4.  Remove the stopwords in text data. Example: Stopwords in English are “the, is, at, on”. There is no single universal list of stopwords used by all NLP tools.
5.  Remove punctuation.
6.  Eliminate extra unnecessary spaces in the text.

```{r, warning = FALSE}
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(syuzhet)
library(ggplot2)
```

```{r}
# warning occurs due to the file does not end with newline
text = readLines('./Data/text.txt')
class(text)
```

```{r}
# after we add new line at the end, no warning will appear
readLines('./Data/newtext.txt')
```

# Transform into Corpus

```{r}
docs = Corpus(VectorSource(text))
#inspect(docs)
```

```{r}
class(docs)
```

## Replace special characters with space

Remove special characters

```{r}
toSpace = content_transformer(function(x, pattern)
  gsub(pattern, '', x))


```

replace

```{r}
docs2 = tm_map(docs, toSpace, '!')
docs3 = tm_map(docs2, toSpace, ':')
docs4 = tm_map(docs3, toSpace, ',')
```

## Convert Capital letter to Small letter

```{r}
docs5 = tm_map(docs4, content_transformer(tolower))
```

## Remove number

```{r}
docs6 = tm_map(docs5, removeNumbers)
```

## Remove stopwords (the, is, at, on)

```{r}
docs7 = tm_map(docs6, removeWords, stopwords('english'))
```

## Remove punctuation

```{r}
docs8 = tm_map(docs7, removePunctuation)
```

## Remove extra spaces

```{r}
docs9 = tm_map(docs8, stripWhitespace)
```

# Text stemming

Find the root word

```{r}
docs10 = tm_map(docs9, stemDocument)
```

# Document term matrix

```{r}
dtm = TermDocumentMatrix(docs10)

m = as.matrix(dtm)

m
```

```{r}
dim(m)
```

There are 162 unique words in 46 line of text

# Word cloud

```{r}
v = sort(rowSums(m), decreasing = T)

v
```

```{r}
d = data.frame(word = names(v), freq = v)

d
```

```{r}
set.seed(12)
wordcloud(words=d$word, freq = d$freq, min.freq = 2, max.words = 150, 
          random.order = F, colors = brewer.pal(8, 'Dark2'))
```

# Word Association

Words associate with freedom

```{r}
findAssocs(dtm, terms = 'freedom', corlimit = 0.3)$freedom
```

relation of words that appear at least 10 times

```{r}
findAssocs(dtm, terms=findFreqTerms(dtm, lowfreq = 10), corlimit = 0.3)
```

# Sentiment Analysis

```{r}
library(sentimentr)
```

```{r}
x = 'Sentiment analysis is super fun'

sentiment(x)
```

0.67 which is positive shows that the sentiment is positive

```{r}
y = 'sentiment analysis is super boring. But I do love working with R'
sentiment(y)
```

```{r}
sentiment(text)
```

Another way to get sentiment score, but both calculate a bit different where:

1.  **syuzhet**: lexicon-based approach, not understand context, count the number of positive/negative in a sentence (e.g., not happy might be count as positive since the word happy is there)
2.  **sentimentr**: more intelligent, can understand context such as "not happy" as negative and not positive.

The only downside to sentimentr is that it will be slow for a huge data.

```{r}
sentiment_text = get_sentiment(text, method='syuzhet') # library(syuzhet)

sentiment_text
```

```{r}
hist(sentiment_text)
```

```{r}
summary(sentiment_text)
```

# Emotion classification

The NRC Emotion Lexicon is a list of English correspond to eight basic emotions:

1.  Anger
2.  Fear
3.  Anticipation
4.  Trust
5.  Surprise
6.  Sadness
7.  Joy
8.  Disgust

```{r}
d2 = get_nrc_sentiment(text)

d2
```

```{r}
td = data.frame(t(d2))
td_new = data.frame(rowSums(td))

td_new
```

```{r}
names(td_new)[1] = 'count'

td_new = cbind('sentiment'=rownames(td_new), td_new)

rownames(td_new) = NULL

qplot(sentiment, weight = count, data=td_new, geom = 'bar',
      fill = sentiment,
      ylab = 'count') + ggtitle('Sentiment Score')
```
